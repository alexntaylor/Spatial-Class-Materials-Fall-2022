vocab_custom$cutoff_pct_best , filter = filter)
vocab_subset
# (3) fit model with "optimal" percentile threshold (i.e. feature subset)
style_model <- stylest_fit(novels_excerpts$text, novels_excerpts$author,
terms = vocab_subset, filter = filter)
# explore output
head(stylest_term_influence(style_model, novels_excerpts$text,
novels_excerpts$author))  # influential terms
authors <- unique(novels_excerpts$author)
term_usage <- style_model$rate
lapply(authors, function(x) head(term_usage[x,][order(-term_usage[x,])])) %>%
setNames(authors)
# odds for known texts
odds <- stylest_odds(style_model, novels_excerpts$text,
novels_excerpts$author)
odds
?stylest_odds
?stylest_model object
?stylest_model
??stylest_model
# (4) predict speaker of a new text
new_text <- emma$text[30:75] %>% paste(., collapse = "")
pred <- stylest_predict(style_model, new_text)
pred$predicted
pred$log_probs
# (4) predict speaker of a new text
new_text <- emma$text[30:100] %>% paste(., collapse = "")
pred <- stylest_predict(style_model, new_text)
pred$predicted
pred$log_probs
# (1) select most informative (discriminative) features
# (subsets vocab by frequency percentile)
# pre-processing choices
filter <- corpus::text_filter(drop_punct = TRUE, drop_number = TRUE)
# why set seed? Replicability
set.seed(1984L)
# fits n-fold cross-validation
vocab_custom <- stylest_select_vocab(novels_excerpts$text,
novels_excerpts$author,
filter = filter, smooth = 1, nfold = 5,
cutoff_pcts = c(25, 50, 75, 99))
# percentile with best prediction rate
vocab_custom$cutoff_pct_best
# rate of incorrectly predicted speakers of held-out texts
vocab_custom$miss_pct
# (2) subset features
# USE SAME FILTER
vocab_subset <- stylest_terms(novels_excerpts$text, novels_excerpts$author,
vocab_custom$cutoff_pct_best , filter = filter)
vocab_subset
# (3) fit model with "optimal" percentile threshold (i.e. feature subset)
style_model <- stylest_fit(novels_excerpts$text, novels_excerpts$author,
terms = vocab_subset, filter = filter)
# explore output
head(stylest_term_influence(style_model, novels_excerpts$text,
novels_excerpts$author))  # influential terms
authors <- unique(novels_excerpts$author)
term_usage <- style_model$rate
lapply(authors, function(x) head(term_usage[x,][order(-term_usage[x,])])) %>%
setNames(authors)
# odds for known texts
odds <- stylest_odds(style_model, novels_excerpts$text,
novels_excerpts$author)
odds
# (4) predict speaker of a new text
new_text <- emma$text[30:100] %>% paste(., collapse = "")
pred <- stylest_predict(style_model, new_text)
pred$predicted
pred$log_probs
# (4) predict speaker of a new text
new_text <- emma$text[30:75] %>% paste(., collapse = "")
pred <- stylest_predict(style_model, new_text)
pred$predicted
pred$log_probs
View(pred)
pred[["predicted"]]
pred[["log_probs"]]
pred
# (4) predict speaker of a new text
new_text <- emma$text[30:75] %>% paste(., collapse = "")
# add more meta field information
emma <- gutenberg_download(gutenberg_id = 158,
mirror="http://mirrors.xmission.com/gutenberg/",
meta_fields = "title")
# (4) predict speaker of a new text
new_text <- emma$text[30:75] %>% paste(., collapse = "")
pred <- stylest_predict(style_model, new_text)
pred$predicted
pred$log_probs
exp(-2.2555)
exp(-1.715)
?stylest_predict
# (4) predict speaker of a new text
new_text <- emma$text[30:1000] %>% paste(., collapse = "")
pred <- stylest_predict(style_model, new_text)
pred$predicted
pred$log_probs
exp(0)
# (4) predict speaker of a new text
new_text <- emma$text[30:100] %>% paste(., collapse = "")
pred <- stylest_predict(style_model, new_text)
pred$predicted
pred$log_probs
# (4) predict speaker of a new text
new_text <- emma$text[30:75] %>% paste(., collapse = "")
pred <- stylest_predict(style_model, new_text)
pred$predicted
pred$log_probs
# (4) predict speaker of a new text
new_text <- emma$text[30:100] %>% paste(., collapse = "")
pred <- stylest_predict(style_model, new_text)
pred$predicted
pred$log_probs
summary(emma$text)
View(emma)
# (4) predict speaker of a new text
new_text <- emma$text[30:16000] %>% paste(., collapse = "")
pred <- stylest_predict(style_model, new_text)
pred$predicted
pred$log_probs
# (4) predict speaker of a new text
new_text <- emma$text %>% paste(., collapse = "")
pred <- stylest_predict(style_model, new_text)
pred$predicted
pred$log_probs
library(titdyverse)
library(tidyverse)
library(stylo)
install.packages("stylo")
library(stylo)
.libPaths()
library(stylo)
library(tidyverse)
library(sf)
library(dodgr)
library(stargazer)
library(units)
# A different problem... Predict traffic flows in Fairfax
net <- weight_streetnet(dodgr_streetnet("fairfax virginia"), wt_profile = "motorcar")
library(tidyverse)
library(sf)
library(stargazer)
library(haven)
library(raster)
library(tidyverse)
library(sf)
library(stargazer)
library(haven)
library(raster)
setwd("/Users/noeljohnson_laptop/Dropbox/Teaching/Spatial Fall 2022/Lectures/Lecture 9/MA_BD_Press")
cities_raw <- read_dta("All_Cities.dta")
# Spatialize the data
EEC <- "+proj=eqdc +lat_0=0 +lon_0=0 +lat_1=43 +lat_2=62 +x_0=0 +y_0=0 +ellps=intl +units=m +no_defs"
cities <- st_as_sf(cities_raw, coords=c("longitude", "latitude"), crs=4326) %>%
st_transform(EEC)
# keep only the variables you want
glimpse(cities)
#cities <- cities %>% dplyr::select(ANY VARIABLES YOU WANT TO KEEP)
cities <- cities %>% filter(countryname != "Oman")
cities <- cities %>% filter(countryname != "Iraq")
cities <- cities %>% filter(countryname != "Saudi Arabia")
cities <- cities %>% filter(countryname != "Yemen")
# Bring in roman roads shape file
rom_roads <- st_read("Europe_Major_Roman_Roads/MajorRomRdsProj.shp") %>%
st_transform(EEC)
glimpse(rom_roads)
rom_roads <- rom_roads %>% dplyr::select()
# Bring in medieval roads shape file
med_roads <- st_read("Europe_Medieval_Trade_Routes/MedRdsProj.shp") %>%
st_transform(EEC)
glimpse(med_roads)
med_roads <- med_roads %>% dplyr::select()
# Bring in rivers shape file
rivers <- st_read("Rivers/Rivers_clippedF.shp") %>%
st_transform(EEC)
glimpse(rivers)
rivers <- rivers %>% dplyr::select()
# Bring in seas shape file
seas <- st_read("Seas/seas_conic.shp") %>%
st_transform(EEC)
glimpse(seas)
seas <- seas %>% dplyr::select()
# Make a bounding box of the cities so you can create one a little bigger
bbox_cities <- st_bbox(cities, crs=EEC)
bbox_cities
xrange <- bbox_cities$xmax - bbox_cities$xmin # range of x values
yrange <- bbox_cities$ymax - bbox_cities$ymin # range of y values
bbox_cities[1] <- bbox_cities[1] - (0.025 * xrange) # xmin - left
bbox_cities[3] <- bbox_cities[3] + (0.025 * xrange) # xmax - right
bbox_cities[2] <- bbox_cities[2] - (0.025 * yrange) # ymin - bottom
bbox_cities[4] <- bbox_cities[4] + (0.025 * yrange) # ymax - top
bbox_cities_sf <- bbox_cities %>%  # take the bounding box ...
st_as_sfc() # ... and make it a sf polygon
bbox_cities_sf
# make grid of 10km cells
grid_sf <- bbox_cities_sf %>% st_make_grid(cellsize = 10000,
what="polygons")
# add IDs to grid, make sf
grid_sf <- st_sf(id = 1:length(grid_sf),
geometry = grid_sf)
st_geometry(grid_sf)
# rom_roads
rom_roads$rom_rd_cost_1 = 0.81
rom_roads$rom_rd_cost_2 = 0.50
rom_roads$rom_rd_cost_3 = 0.81
rom_roads$rom_rd_cost_4 = 0.50
# med_roads
med_roads$med_rd_cost_1 = 0.81
med_roads$med_rd_cost_2 = 0.50
med_roads$med_rd_cost_3 = 0.81
med_roads$med_rd_cost_4 = 0.50
# rivers
rivers$river_cost_1 = 0.21
rivers$river_cost_2 = 0.50
rivers$river_cost_3 = 0.51
rivers$river_cost_4 = 0.24
# seas
seas$seas_cost_1 = 0.08
seas$seas_cost_2 = 0.13
seas$seas_cost_3 = 0.10
seas$seas_cost_4 = 0.18
# add a portage value to the grid file...
grid_sf$portage_cost_1 = 1.00
grid_sf$portage_cost_2 = 1.00
grid_sf$portage_cost_3 = 1.00
grid_sf$portage_cost_4 = 1.00
# Sequentially st_join the data sets into the cities file...
grid_sf <- grid_sf %>%
st_join(rom_roads) %>%
st_join(med_roads) %>%
st_join(rivers) %>%
st_join(seas)
library(tidyverse)
library(sf)
library(stargazer)
library(haven)
library(raster)
setwd("/Users/noeljohnson_laptop/Dropbox/Teaching/Spatial Fall 2022/Lectures/Lecture 9/MA_BD_Press")
cities_raw <- read_dta("All_Cities.dta")
# Spatialize the data
EEC <- "+proj=eqdc +lat_0=0 +lon_0=0 +lat_1=43 +lat_2=62 +x_0=0 +y_0=0 +ellps=intl +units=m +no_defs"
cities <- st_as_sf(cities_raw, coords=c("longitude", "latitude"), crs=4326) %>%
st_transform(EEC)
# keep only the variables you want
glimpse(cities)
#cities <- cities %>% dplyr::select(ANY VARIABLES YOU WANT TO KEEP)
cities <- cities %>% filter(countryname != "Oman")
cities <- cities %>% filter(countryname != "Iraq")
cities <- cities %>% filter(countryname != "Saudi Arabia")
cities <- cities %>% filter(countryname != "Yemen")
# Bring in roman roads shape file
rom_roads <- st_read("Europe_Major_Roman_Roads/MajorRomRdsProj.shp") %>%
st_transform(EEC)
glimpse(rom_roads)
rom_roads <- rom_roads %>% dplyr::select()
# Bring in medieval roads shape file
med_roads <- st_read("Europe_Medieval_Trade_Routes/MedRdsProj.shp") %>%
st_transform(EEC)
glimpse(med_roads)
med_roads <- med_roads %>% dplyr::select()
# Bring in rivers shape file
rivers <- st_read("Rivers/Rivers_clippedF.shp") %>%
st_transform(EEC)
glimpse(rivers)
rivers <- rivers %>% dplyr::select()
# Bring in seas shape file
seas <- st_read("Seas/seas_conic.shp") %>%
st_transform(EEC)
glimpse(seas)
seas <- seas %>% dplyr::select()
# Make a bounding box of the cities so you can create one a little bigger
bbox_cities <- st_bbox(cities, crs=EEC)
bbox_cities
xrange <- bbox_cities$xmax - bbox_cities$xmin # range of x values
yrange <- bbox_cities$ymax - bbox_cities$ymin # range of y values
bbox_cities[1] <- bbox_cities[1] - (0.025 * xrange) # xmin - left
bbox_cities[3] <- bbox_cities[3] + (0.025 * xrange) # xmax - right
bbox_cities[2] <- bbox_cities[2] - (0.025 * yrange) # ymin - bottom
bbox_cities[4] <- bbox_cities[4] + (0.025 * yrange) # ymax - top
bbox_cities_sf <- bbox_cities %>%  # take the bounding box ...
st_as_sfc() # ... and make it a sf polygon
bbox_cities_sf
# make grid of 10km cells
grid_sf <- bbox_cities_sf %>% st_make_grid(cellsize = 10000,
what="polygons")
# add IDs to grid, make sf
grid_sf <- st_sf(id = 1:length(grid_sf),
geometry = grid_sf)
st_geometry(grid_sf)
# rom_roads
rom_roads$rom_rd_cost_1 = 0.81
library(tidyverse)
library(sf)
library(stargazer)
library(haven)
library(raster)
setwd("/Users/noeljohnson_laptop/Dropbox/Teaching/Spatial Fall 2022/Lectures/Lecture 9/MA_BD_Press")
cities_raw <- read_dta("All_Cities.dta")
# Spatialize the data
EEC <- "+proj=eqdc +lat_0=0 +lon_0=0 +lat_1=43 +lat_2=62 +x_0=0 +y_0=0 +ellps=intl +units=m +no_defs"
cities <- st_as_sf(cities_raw, coords=c("longitude", "latitude"), crs=4326) %>%
st_transform(EEC)
# keep only the variables you want
glimpse(cities)
#cities <- cities %>% dplyr::select(ANY VARIABLES YOU WANT TO KEEP)
cities <- cities %>% filter(countryname != "Oman")
cities <- cities %>% filter(countryname != "Iraq")
cities <- cities %>% filter(countryname != "Saudi Arabia")
cities <- cities %>% filter(countryname != "Yemen")
# Bring in roman roads shape file
rom_roads <- st_read("Europe_Major_Roman_Roads/MajorRomRdsProj.shp") %>%
st_transform(EEC)
glimpse(rom_roads)
rom_roads <- rom_roads %>% dplyr::select()
# Bring in medieval roads shape file
med_roads <- st_read("Europe_Medieval_Trade_Routes/MedRdsProj.shp") %>%
st_transform(EEC)
glimpse(med_roads)
med_roads <- med_roads %>% dplyr::select()
# Bring in rivers shape file
rivers <- st_read("Rivers/Rivers_clippedF.shp") %>%
st_transform(EEC)
glimpse(rivers)
rivers <- rivers %>% dplyr::select()
# Bring in seas shape file
seas <- st_read("Seas/seas_conic.shp") %>%
st_transform(EEC)
glimpse(seas)
seas <- seas %>% dplyr::select()
# Make a bounding box of the cities so you can create one a little bigger
bbox_cities <- st_bbox(cities, crs=EEC)
bbox_cities
xrange <- bbox_cities$xmax - bbox_cities$xmin # range of x values
yrange <- bbox_cities$ymax - bbox_cities$ymin # range of y values
bbox_cities[1] <- bbox_cities[1] - (0.025 * xrange) # xmin - left
bbox_cities[3] <- bbox_cities[3] + (0.025 * xrange) # xmax - right
bbox_cities[2] <- bbox_cities[2] - (0.025 * yrange) # ymin - bottom
bbox_cities[4] <- bbox_cities[4] + (0.025 * yrange) # ymax - top
bbox_cities_sf <- bbox_cities %>%  # take the bounding box ...
st_as_sfc() # ... and make it a sf polygon
bbox_cities_sf
# make grid of 10km cells
grid_sf <- bbox_cities_sf %>% st_make_grid(cellsize = 10000,
what="polygons")
# add IDs to grid, make sf
grid_sf <- st_sf(id = 1:length(grid_sf),
geometry = grid_sf)
st_geometry(grid_sf)
# rom_roads
rom_roads$rom_rd_cost_1 = 0.81
# med_roads
med_roads$med_rd_cost_1 = 0.81
# rivers
rivers$river_cost_1 = 0.21
# seas
seas$seas_cost_1 = 0.08
# add a portage value to the grid file...
grid_sf$portage_cost_1 = 1.00
# Sequentially st_join the data sets into the cities file...
grid_sf <- grid_sf %>%
st_join(rom_roads) %>%
st_join(med_roads) %>%
st_join(rivers) %>%
st_join(seas)
library(tidyverse)
library(sf)
library(dodgr)
library(stargazer)
library(units)
# # A different problem... Predict traffic flows in Fairfax
net <- weight_streetnet(dodgr_streetnet("fairfax virginia"), wt_profile = "motorcar")
# # A different problem... Predict traffic flows in Fairfax
location <- osmdata::getbb ("fairfax virginia")
net <- weight_streetnet(dodgr_streetnet(location), wt_profile = "motorcar")
nodes <- dodgr_vertices(net) # get nodes of vertices of network
# # A different problem... Predict traffic flows in Fairfax
location <- osmdata::getbb ("bethesda maryland")
net <- weight_streetnet(dodgr_streetnet(location), wt_profile = "motorcar")
library(tidyverse)
library(sf)
library(dodgr)
library(stargazer)
library(units)
# starbucks across from merten hall
xy <- rbind (c( -77.30724741044384, 38.83382348516032),
# carow hall
c( -77.30124991245012, 38.83144057919348))
xy <- data.frame(lon = xy[, 1], lat = xy[, 2])
# retrieves the OSM network
haj <- dodgr_streetnet(pts = xy,
expand = 0.1, quiet = F) %>%
select(highway, osm_id)
# turns it into a graph
haj.graph <- weight_streetnet(haj,
wt_profile = "foot")
# computes the weighted distance on the graph
test <- dodgr_dists(haj.graph, from = xy[1,], to = xy[2,])
plot(haj[3], reset=T)
box <- st_bbox(haj)
f <- dodgr_flows_aggregate(haj.graph, from = xy[1,], to = xy[2,], flows=1, contract= T)
# the call "merge_directed_flows()" seems to no longer work. (12/14/2020)
# dodgr_flowmap(merge_directed_flows(f), linescale=5, bbox=box)
# plot(haj[3], add=T)
# these two lines do seem to work...
dodgr_flowmap(f, linescale=5, bbox=box)
plot(haj[3], add=T)
# # A different problem... Predict traffic flows in Fairfax
location <- osmdata::getbb ("fairfax virginia")
net <- weight_streetnet(dodgr_streetnet(location), wt_profile = "motorcar")
# # A different problem... Predict traffic flows in Fairfax
location <- osmdata::getbb ("20817 bethesda")
net <- weight_streetnet(dodgr_streetnet(location), wt_profile = "motorcar")
net <- weight_streetnet(dodgr_streetnet(location), wt_profile = "motorcar")
# # A different problem... Predict traffic flows in Fairfax
location <- osmdata::getbb ("fairfax virginia")
net <- weight_streetnet(dodgr_streetnet(location), wt_profile = "motorcar")
install.packages("osmdata")
install.packages("osmdata")
install.packages("osmdata")
install.packages("osmdata")
install.packages("osmdata")
install.packages("osmdata")
install.packages("osmdata")
install.packages("osmdata")
install.packages("osmdata")
install.packages("osmdata")
install.packages("osmdata")
install.packages("osmdata")
library(tidyverse)
library(sf)
library(dodgr)
library(stargazer)
library(units)
library(osmdata)
# starbucks across from merten hall
xy <- rbind (c( -77.30724741044384, 38.83382348516032),
# carow hall
c( -77.30124991245012, 38.83144057919348))
xy <- data.frame(lon = xy[, 1], lat = xy[, 2])
# retrieves the OSM network
haj <- dodgr_streetnet(pts = xy,
expand = 0.1, quiet = F) %>%
select(highway, osm_id)
# turns it into a graph
haj.graph <- weight_streetnet(haj,
wt_profile = "foot")
# computes the weighted distance on the graph
test <- dodgr_dists(haj.graph, from = xy[1,], to = xy[2,])
plot(haj[3], reset=T)
box <- st_bbox(haj)
f <- dodgr_flows_aggregate(haj.graph, from = xy[1,], to = xy[2,], flows=1, contract= T)
# the call "merge_directed_flows()" seems to no longer work. (12/14/2020)
# dodgr_flowmap(merge_directed_flows(f), linescale=5, bbox=box)
# plot(haj[3], add=T)
# these two lines do seem to work...
dodgr_flowmap(f, linescale=5, bbox=box)
plot(haj[3], add=T)
# # A different problem... Predict traffic flows in Fairfax
location <- osmdata::getbb ("fairfax virginia")
net <- weight_streetnet(dodgr_streetnet(location), wt_profile = "motorcar")
net <- weight_streetnet(dodgr_streetnet(location), wt_profile = "motorcar")
# # A different problem... Predict traffic flows in Fairfax
location <- osmdata::getbb ("fairfax virginia")
net <- weight_streetnet(dodgr_streetnet(location), wt_profile = "motorcar")
library(tidyverse)
library(sf)
library(dodgr)
library(stargazer)
library(units)
library(osmdata)
net <- weight_streetnet(dodgr_streetnet(location), wt_profile = "motorcar")
library(tidyverse)
library(sf)
library(dodgr)
library(stargazer)
library(units)
library(osmdata)
# starbucks across from merten hall
xy <- rbind (c( -77.30724741044384, 38.83382348516032),
# carow hall
c( -77.30124991245012, 38.83144057919348))
xy <- data.frame(lon = xy[, 1], lat = xy[, 2])
# retrieves the OSM network
haj <- dodgr_streetnet(pts = xy,
expand = 0.1, quiet = F) %>%
select(highway, osm_id)
# turns it into a graph
haj.graph <- weight_streetnet(haj,
wt_profile = "foot")
# computes the weighted distance on the graph
test <- dodgr_dists(haj.graph, from = xy[1,], to = xy[2,])
plot(haj[3], reset=T)
box <- st_bbox(haj)
f <- dodgr_flows_aggregate(haj.graph, from = xy[1,], to = xy[2,], flows=1, contract= T)
# the call "merge_directed_flows()" seems to no longer work. (12/14/2020)
# dodgr_flowmap(merge_directed_flows(f), linescale=5, bbox=box)
# plot(haj[3], add=T)
# these two lines do seem to work...
dodgr_flowmap(f, linescale=5, bbox=box)
plot(haj[3], add=T)
# # A different problem... Predict traffic flows in Fairfax
location <- osmdata::getbb ("fairfax virginia")
net <- weight_streetnet(dodgr_streetnet(location), wt_profile = "motorcar")
library(osmextract)
install(osmextract)
install.packages("osmextract")
library(osmextract)
cycleways_england = oe_get(
"England",
quiet = FALSE,
query = "SELECT * FROM 'lines' WHERE highway = 'cycleway'"
)
